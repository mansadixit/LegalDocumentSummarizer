# -*- coding: utf-8 -*-
"""bartxsum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BPTuYkcKVZ2vsIv1_LBoWfzJWtsr-0gi
"""

!pip install transformers torch tqdm pandas

import pandas as pd
from transformers import BartTokenizer, BartForConditionalGeneration

import torch
from tqdm import tqdm

# Import Pandas library
import pandas as pd
# Upload CSV file from your device
from google.colab import files
uploaded = files.upload()

# Assuming the file is named 'legal_data.csv'
file_name = list(uploaded.keys())[0]

# Read the CSV file into a Pandas DataFrame
df = pd.read_csv(file_name)

"""# New section"""

df = df.dropna(subset=['case_text']).reset_index(drop=True)

from transformers import BartTokenizer, BartForConditionalGeneration
import torch

!pip install transformers

from transformers import BartTokenizer, BartForConditionalGeneration

model_name = "AdamCodd/bart-large-xsum-samsum"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

!pip install rouge-score

import pandas as pd
import nltk
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from tqdm import tqdm
from transformers import BartTokenizer, BartForConditionalGeneration
import torch

ref_summaries = df['case_text'].iloc[2:100].tolist()

rouge_scores = []
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
bleu_scores = []

from tqdm import tqdm
import nltk
from nltk.translate.bleu_score import sentence_bleu

ref_index = None
import nltk
nltk.download('punkt')
for index, row in tqdm(df.iterrows(), total=len(df)):
    case_text = row['case_text']

    inputs = tokenizer.encode("summarize: " + case_text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    if index >= 2 and index < 100:
        ref_index = index - 2
        rouge_score = scorer.score(ref_summaries[ref_index], summary)
        rouge_scores.append(rouge_score)

    if ref_index is not None:
        ref_tokenized = nltk.word_tokenize(ref_summaries[ref_index])
        gen_tokenized = nltk.word_tokenize(summary)
        bleu_score = sentence_bleu([ref_tokenized], gen_tokenized)
        bleu_scores.append(bleu_score)

average_rouge_scores = [sum(score['rouge1'][2] for score in rouge_scores) / len(rouge_scores),
                       sum(score['rouge2'][2] for score in rouge_scores) / len(rouge_scores),
                       sum(score['rougeL'][2] for score in rouge_scores) / len(rouge_scores)]

average_bleu_score = sum(bleu_scores) / len(bleu_scores)

print("Average ROUGE Scores (ROUGE-1, ROUGE-2, ROUGE-L):", average_rouge_scores)
print("Average BLEU Score:", average_bleu_score)



